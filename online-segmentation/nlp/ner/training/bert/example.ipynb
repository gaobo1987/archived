{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained model path:  /home/bo/workspace/online-segmentation/nlp/ner/resources/el/models/bert_base_greek_uncased_ft\n",
      "data path:  /home/bo/workspace/online-segmentation/nlp/ner/resources/el/data/spacyner2018/train.txt\n",
      "output path:  /home/bo/workspace/online-segmentation/nlp/ner/training/bert/output\n",
      "spacy path:  /home/bo/workspace/online-segmentation/nlp/ner/resources/el/models/spacy_default\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('..', '..', '..', '..')))\n",
    "from nlp.bert.bert_model import BertModel\n",
    "import spacy\n",
    "\n",
    "lang = 'el'\n",
    "# =======================================================\n",
    "# model_dirname examples:\n",
    "# bert_base_greek_uncased\n",
    "# bert_base_multilingual_uncased\n",
    "# =======================================================\n",
    "model_dirname = 'bert_base_greek_uncased'\n",
    "# =======================================================\n",
    "# data_dirname examples:\n",
    "# (nl) demorgen2000\n",
    "# (el) spacyner2018\n",
    "# =======================================================\n",
    "data_dirname = 'spacyner2018'\n",
    "\n",
    "\n",
    "pretrained_model_path = \\\n",
    "    os.path.abspath(os.path.join('..', '..', '..', 'bert', 'pretrained', model_dirname))\n",
    "pretrained_model_path = \\\n",
    "    os.path.abspath(os.path.join('..', '..', 'resources', lang, 'models', 'bert_base_greek_uncased_ft'))\n",
    "data_path = \\\n",
    "    os.path.abspath(os.path.join('..', '..', 'resources', lang, 'data', data_dirname, 'train.txt'))\n",
    "output_path = \\\n",
    "    os.path.abspath(os.path.join('..', '..', 'training', 'bert', 'output'))\n",
    "spacy_path = \\\n",
    "    os.path.abspath(os.path.join('..', '..', 'resources', lang, 'models', 'spacy_default'))\n",
    "print('pretrained model path: ', pretrained_model_path)\n",
    "print('data path: ', data_path)\n",
    "print('output path: ', output_path)\n",
    "print('spacy path: ', spacy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== BERT model ==========\n",
      "model_path:  /home/bo/workspace/online-segmentation/nlp/ner/resources/el/models/bert_base_greek_uncased_ft\n",
      "tokenizer_args:  {'do_lower_case': True, 'strip_accents': True, 'keep_accents': False, 'use_fast': False}\n",
      "entity_labels:  ['B-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'O', 'I-LOC', 'I-PER', 'B-MISC', 'I-MISC']\n",
      "bert_type:  bert\n",
      "num_epochs:  10\n",
      "batch_size:  32\n",
      "max_seq_length:  128\n",
      "learning_rate:  1e-06\n",
      "adam_epsilon:  1e-08\n",
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# to use uncased tokenizer\n",
    "tokenizer_args = {\n",
    "    'do_lower_case': True,\n",
    "    'strip_accents': True,\n",
    "    'keep_accents': False\n",
    "}\n",
    "bert_model = BertModel(pretrained_model_path, bert_type='bert', \n",
    "                       tokenizer_args=tokenizer_args, num_epochs=10, \n",
    "                       learning_rate=1e-6,\n",
    "                       batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_global_step, _tr_loss = bert_model.train(data_path, output_path)\n",
    "print(f'training finished, global_step={_global_step}, tr_loss={_tr_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== BERT model ==========\n",
      "model_path:  /home/bo/workspace/online-segmentation/nlp/ner/training/bert/output/bert-checkpoint-510\n",
      "tokenizer_args:  {'do_lower_case': False, 'strip_accents': True, 'keep_accents': False, 'use_fast': False}\n",
      "entity_labels:  ['B-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'O', 'I-LOC', 'I-PER', 'B-MISC', 'I-MISC']\n",
      "bert_type:  bert\n",
      "num_epochs:  1\n",
      "batch_size:  32\n",
      "max_seq_length:  128\n",
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# verify training\n",
    "nlp = spacy.load(spacy_path)\n",
    "# load the newly trained model\n",
    "new_model_path = os.path.abspath(os.path.join('output', 'bert-checkpoint-510'))\n",
    "# new_model_path = '/home/bo/workspace/online-segmentation/ner/evaluation/nl/models/bert_base_dutch_cased'\n",
    "new_model = BertModel(new_model_path, bert_type='bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words\n",
      "['σε', 'δήλωσή', 'του', ',', 'ο', 'κ.', 'μίχαλος', 'εξέφρασε', 'την', 'οδύνη', 'του', 'εβεα', 'για', 'τις', 'ανθρώπινες', 'ζωές', 'που', 'χάθηκαν', ',', 'τονίζοντας', 'πως', 'η', 'τραγωδία', 'επισκιάζει', 'τα', 'πάντα', '.']\n",
      "predictions:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# from parliamentary data\n",
    "s = 'Hij bezocht Amsterdam.'\n",
    "s = 'Mark Rutte is een Nederlands politicus.'\n",
    "s = 'Μπορεί να έχασαν όλες τις εκπλήξεις και τις συγκινήσεις που μας χάρισε το Παγκόσμιο Κύπελλο στη Ρωσία μέχρι τώρα , αλλά οι δύο τελικοί δεν χάνονται με τίποτα! '\n",
    "s = 'Η Αθήνα, Ελλάδα είναι ένα όμορφο μέρος.'\n",
    "s = 'Σε δήλωσή του , ο κ. Μίχαλος εξέφρασε την οδύνη του ΕΒΕΑ για τις ανθρώπινες ζωές που χάθηκαν , τονίζοντας πως η τραγωδία επισκιάζει τα πάντα.'\n",
    "words = [t.text for t in nlp(s.lower())]\n",
    "print('words')\n",
    "print(words)\n",
    "preds = new_model.ner(words)\n",
    "print('predictions:')\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for n_iter in range(100):\n",
    "    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qsegmt-venv",
   "language": "python",
   "name": "qsegmt-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
